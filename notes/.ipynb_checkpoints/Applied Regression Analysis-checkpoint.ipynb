{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Analysis\n",
    "- Investigate dataset\n",
    "-- pairs() for multiple linear regression, plot() for simple linear reg.\n",
    "\n",
    "1. y = lm(response~.,df)\n",
    "2. Test Overall Goodness of Fit, alpha = 0.05\n",
    "- 1. Ho = B1=B2=B3=B4=0 or H1= not all Coeffs = 0\n",
    "- 2. P-value: if p-value < 0.05 then we reject the Null (Ho)\n",
    "- 3. Look at significance of predictors\n",
    "- 4. ANOVA F-Stat\n",
    "-- SSE = sum((linearmodel$residuals)^2) #Sum Squares Error\n",
    "--- Sum of suqared deviations about the least squeares line.\n",
    "--- How  much of variation in Y is left unexplained by the model\n",
    "--- How much cannot be attributed to a linear relationship\n",
    "-- Residual Std Error = sqrt(sse/(n-p-r))\n",
    "-- SST = sum((y-mean(y))^2)\n",
    "--- Sum of squared deviations about the sample mean of the observed y values (no predictors taken into account)\n",
    "-- SSR = SST - SSE\n",
    "--- Proporton of observed Y variation that can be explained by the simple linear regression model\n",
    "-- F-stat = (SSR/no_predictors)/(SSE/observations)\n",
    "- 5. Analyze R^2 and R^2 Adjusted\n",
    "-- SSR/SST\n",
    "3. Run Stepwise Function\n",
    "4. Run anova(linearmodel, reduced_linearmodel) to see which is a better model\n",
    "-- Ho: g and g_red are significantly the same (g_red is better)\n",
    "-- Ha: r and g_red are significantly different\n",
    "--- Read P-value of both models (should be < alpha), check SSE (should decrease), R^2 (should increase), and change in coefficients (should not change much)\n",
    "\n",
    "Predicted Values\n",
    "- predict(bestlinearmodel, newdf, se=TRUE, interval=\"prediction\")\n",
    "\n",
    "Confidence Interval\n",
    "- predict.lm(bestlinearmodel, newdf, interval=\"confidence\", level=0.95)\n",
    "- Basically, predicted value from above output +/- (1.96 * se from above output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing Predictors for Better Linear Model\n",
    "\n",
    "A. Principal Component Analysis (PCA)\n",
    "Shows the decomposition of the cariations among the existing predictors. Predictors with small variations can be discarded.\n",
    "1. The eigenvectors are orthogonal (perpendicular) and normalized (length = 1)\n",
    "2. The eigensystem of corr of dataset is the same as YtY after centering and scaling\n",
    "3. princomp() or prcomp() give an analysis of which variables among all existing ones, count the majority of the variation.\n",
    "-- Proportion of Variance shows how much variance is accounted for by each of the components\n",
    "-- The eigenvalues/eigenvectors of the correlation matrix are the same as in the PCA output\n",
    "-- Use plot.(pca, type=\"l\") to see the elbow plot and which values can be dropped\n",
    "\n",
    "B. Multicollinearity\n",
    "Collinearity problems can be inspected by\n",
    "1. The correlation matrix\n",
    "2. Scatterplots amongst predictors\n",
    "3. The eigenvalues of XtX\n",
    "-- x <- model.matrix(linearmodel)\n",
    "-- e <- eigen(t(x)%*%x) # eigenvalues and associated eigenvectors of t(x)*x\n",
    "-- signif(e$values,3)\n",
    "--- plot(e$values, type=\"l\")\n",
    "-- eigenvalues must be small and almost the same, none must be zero. That doesn't mean they can't cause problems in evaluating the coefficient beta\n",
    "4. Condition Number\n",
    "-- sqrt(highest eigenvalue/lowest eigenvalue)\n",
    "-- if > 15 there is a concern for collinearity\n",
    "5. VIF coefficients (Variance Inflation Factor)\n",
    "-- vif(linearmodel)\n",
    "-- if > 10 then there is a concern for collinearity\n",
    "\n",
    "C. Power Transformations - Log Transformations\n",
    "1. Check scatterplot matrix\n",
    "-- pairs(df, pch=10)\n",
    "-- If there exists curving pattern or part of the data clumps in to one location then the variables (and maybe the predictor) might need transformations\n",
    "2. Boxcox = Transformation on response only\n",
    "-- boxcox(linearmodel, lambda=seq(0.0,1.0,0.1), plotit=TRUE)\n",
    "-- inverse.response.plot(linearmodel,key=TRUE)\n",
    "-- Lambda values are provided. Choose the one that gives the least SSE (RSS)\n",
    "--- if lambda = 1 then log()\n",
    "-- Incorporate to new linear model\n",
    "3. PowerTransform - Transforms both predictors and response\n",
    "-- powerTransform(cbind(response+variable)~1, df)\n",
    "-- Use \"Rounded Pwr\" to get lambda value\n",
    "-- 0.5 = sqrt(), 0 = log()\n",
    "D. Polynomial Regression\n",
    "-- poly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factors and Levels in Applied Regression Analysis\n",
    "\n",
    "- lapply(split(df,factor),summary) #apply summary to all factors\n",
    "- tapply(resp,factor,function) #e.g. mean, sd\n",
    "- boxplot(resp~factor)\n",
    "- lm(resp~predictor - 1, df) #remove intercept to see dummy variables\n",
    "1. Compare models with interaction and without interaction using anova()\n",
    "- interaction: lm(resp~pred:factor) #if there is one factor\n",
    "- interaction: lm(resp~pred*factor) #to see all combs of interaction\n",
    "2. Check interaction between 2 factor columns in df\n",
    "- interaction.plot(x.factor,trace.factor,resp,df\n",
    "- example of interaction model: lm(resp~(f1+f2)^2,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual Analysis\n",
    "\n",
    "- g = lm(r~p,df)\n",
    "- X = model.matrix(g)\n",
    "- Xt_X_inv = solve(t(X) %*% X) #inverse of transposed X * X\n",
    "- H = X %*% Xt_X_inv %*% X #hat matrix for g\n",
    "- y = df$pred\n",
    "- res = y - H%*%y # residuals\n",
    "\n",
    "1. Investigate Raw Residuals and Standardized Residuals\n",
    "- par(mfrow=c(2,1))\n",
    "- res = g$residuals #Raw residuals\n",
    "- sres = rstandard(g) #Standardized residuals - residual divided by its standard deviation\n",
    "- plot(res);plot(sres)\n",
    "\n",
    "2. Check for outliers\n",
    "a. Leverages\n",
    "- lev = hatvalues(g) #lev are the diagonal entries of the hat matrix\n",
    "- plot(lev,main=\"Index plot of leverages\")\n",
    "- abline(h=2(p+1)/n) #large leverage, not good, big diff b/w y and yhat, there are outliers, leverage criterion = abline\n",
    "- levnames = rownames(df) #assign names to lev so we know which to drop\n",
    "- names(lev) <- levnames\n",
    "- lev[lev > h]\n",
    "\n",
    "b. Cooks Distance\n",
    "- cook = cooks.distance(g)\n",
    "- plot(cook, ylab=\"Cooks Distance\") #plot cooks distance\n",
    "- identify(1:50,cook,levnames) # click on points and names will appear\n",
    "Similar to Leverages, look for outliers in residuals and drop from df\n",
    "\n",
    "c. Halfnorm\n",
    "halfnorm(cook,4) # creates a half normal plot\n",
    "\n",
    "3. Residual Analysis\n",
    "Check model adequacy. Assumption of independent and identically normal distributed errors with 0 mean and constant variance\n",
    "a. Residuals vs fitted values plot\n",
    "plot(g$fitted,g$residuals); abline(h=0) #should look random, no patterns\n",
    "When the residuals center on zero, they indicate that the modelâ€™s predictions are correct on average rather than systematically too high or low.\n",
    "b. Normality: QQ Plot\n",
    "qqnorm(g$residuals)\n",
    "qqline(g$residuals)\n",
    "c. Normality: Histogram\n",
    "hist(g$residuals)\n",
    "d. Normality: Shapiro Test\n",
    "shapiro.test(g$residuals)\n",
    "-- Ho: residuals are normal Ha: residuals are not normal\n",
    "-- if p-value < alpha, reject the Ho (null) (we want high p-val in this test bc we would fail to reject null which means res are normal)\n",
    "e. Check correlation among residuals: Durbin-Watson Test\n",
    "install.packages(\"lmtest\")\n",
    "dwtest(g,alternative=\"two-sided\")\n",
    "Ho= Correlation = 0, there is no correlation among residuals\n",
    "Ha= Correlation != 0, there is correlation amongst residuals\n",
    "if p-val < alpha then we reject the null\n",
    "(we want high p-val so we can fail to reject the null and conclude that there is no correlation among residuals)\n",
    "\n",
    "-- If residuals don't look random or there is correlation among residuals, the following can be missing:\n",
    "a. Predictor\n",
    "b. Interaction between predictors\n",
    "c. Polynomial or other transformations\n",
    "(https://statisticsbyjim.com/regression/check-residual-plots-regression-analysis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
